{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Only run this cell if you are using Google Colab\n",
    "## Skip with docker container\n",
    "## Technically you can run these commands on your local (linux distro with apt) machine to have things installed in local environment, preferrably using conda or python venv, but overall not recommended\n",
    "\n",
    "!apt-get update && apt-get install swig cmake ffmpeg freeglut3-dev xvfb\n",
    "!git clone https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "%cd /content/rl-baselines3-zoo/\n",
    "%pip install -r requirements.txt\n",
    "%cd /content/\n",
    "!git clone https://github.com/yusenz/gym-maze.git\n",
    "%cd /content/gym-maze\n",
    "%pip install .\n",
    "%cd /content/\n",
    "%pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity testing to see if you have gpu setup correctly\n",
    "# Should display True, 0, and the name of your GPU or some Tesla device on Colab\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Toy environment recap\n",
    "A simple 2D maze environment where an agent (blue dot) finds its way from the top left corner (blue square) to the goal at the bottom right corner (red square). \n",
    "The objective is to find the shortest path from the start to the goal.\n",
    "\n",
    "<kbd>![Simple 2D maze environment](http://i.giphy.com/Ar3aKxkAAh3y0.gif)</kbd>\n",
    "\n",
    "#### Action space\n",
    "The agent may only choose to go up, down, right, or left (\"N\", \"S\", \"E\", \"W\"; or alternatively: 0, 1, 2, 3). If the way is blocked, it will remain at the same the location. \n",
    "\n",
    "#### Observation space\n",
    "The observation space is the (x, y) coordinate of the agent. The top left cell is `[0. 0.]`, and the bottom right cell is `[size-1, size-1]`.\n",
    "\n",
    "#### Reward\n",
    "A reward of 1 is given when the agent reaches the goal. For every step in the maze, the agent recieves a reward of -0.1/(number of cells). This penalty can be adjusted by the `penalty=0.1` parameter, and the normalization is controlled by `penalty_normalize='size'`. I have implemented a few other normalizations: 'none', 'sqrt_size', 'log_size'. You can use `env = gym.make('maze-sample-10x10-v0',penalty=penalty,penalty_normalize=penalty_normalize)` to change the penalty and normalization.\n",
    "\n",
    "#### End condition\n",
    "The maze is reset when the agent reaches the goal (terminated), or maximum time step (10,000 for 10x10 maze) is reached (truncated). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: The agent\n",
    "Again, this block of code defines the base agent. I saved `self.terminated` and `self.truncated` in the agent as well, so that the value can be checked in `update_step()` without changing the calling interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "class BaseAgent:\n",
    "    def __init__(self, env, verbose=1, ran_seed=42):\n",
    "        self.env = env\n",
    "        # random seed is only set once when the agent is initialized\n",
    "        self.env.seed(ran_seed)\n",
    "        self.env.action_space.seed(ran_seed+1)  # why isnt this set at env.seed?\n",
    "        self.env.observation_space.seed(ran_seed+2)\n",
    "        self.random_state = np.random.RandomState(ran_seed+3)\n",
    "        self.observation_space = env.observation_space\n",
    "        space = gym.spaces.utils.flatten_space(self.observation_space)\n",
    "        # these values reflect the return value of each step, not cumulative\n",
    "        self.terminated = False\n",
    "        self.truncated = False\n",
    "        self.info = None\n",
    "        # sanitize the observation space\n",
    "        if isinstance(space, gym.spaces.Box):\n",
    "            if space.shape is not None and len(space.shape) > 0:\n",
    "                if space.is_bounded(\"both\"):\n",
    "                    high = space.high\n",
    "                    low = space.low\n",
    "                    self.observation_space_shape = high - low + 1\n",
    "        self.action_space = env.action_space\n",
    "        space = gym.spaces.utils.flatten_space(self.action_space)\n",
    "        # sanitize the action space\n",
    "        if isinstance(space, gym.spaces.Box):\n",
    "            if space.shape is not None and len(space.shape) > 0:\n",
    "                if space.is_bounded(\"both\"):\n",
    "                    high = space.high\n",
    "                    low = space.low\n",
    "                    self.action_space_shape = high - low + 1\n",
    "        self.verbose = verbose\n",
    "        self.cumulative_reward = 0\n",
    "        self.num_steps = 0\n",
    "    def select_action(self, state):\n",
    "        raise NotImplementedError\n",
    "    def update_step(self, reward: float):\n",
    "        self.cumulative_reward += reward\n",
    "        self.num_steps += 1\n",
    "    def update_episode(self):\n",
    "        self.reset_episode()\n",
    "    def update_rollout(self):\n",
    "        if self.verbose > 0:\n",
    "            print('update_rollout in base class is called, nothing is changed')\n",
    "    def update_replay(self):\n",
    "        if self.verbose > 0:\n",
    "            print('update_replay in base class is called, nothing is changed')\n",
    "    def reset_episode(self):\n",
    "        self.cumulative_reward = 0\n",
    "        self.num_steps = 0\n",
    "        self.terminated = False\n",
    "        self.truncated = False\n",
    "        self.info = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(BaseAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.cumulative_reward = 0\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def select_action(self, state):\n",
    "        action = self.action_space.sample()\n",
    "        if self.verbose > 1:\n",
    "            print('Random agent selected action: ', action)\n",
    "        return action\n",
    "    def update_step(self, old_state, action, reward, new_state):\n",
    "        super().update_step(reward)\n",
    "    def update_episode(self, terminated, truncated):\n",
    "        if self.verbose > 0:\n",
    "            if terminated:\n",
    "                print('Episode terminated')\n",
    "            if truncated:\n",
    "                print('Episode truncated')\n",
    "        super().update_episode()\n",
    "    def update_rollout(self):\n",
    "        pass\n",
    "    def update_replay(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: The training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_maze\n",
    "import numpy as np\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as display\n",
    "import cv2\n",
    "\n",
    "\n",
    "def main_loop(agent, args):\n",
    "    ## experiment parameters\n",
    "    # Training terminates on either reaching NUM_EPISODES or MAX_STEPS_TOTAL\n",
    "    NUM_EPISODES = args['NUM_EPISODES']\n",
    "    MAX_STEPS_TOTAL = args['MAX_STEPS_TOTAL']\n",
    "    RENDER_MAZE = args['RENDER_MAZE']\n",
    "    RENDER_EVERY = args['RENDER_EVERY']  # render every RENDER_EVERY episode because rendering can be slow\n",
    "    verbose = args['verbose']\n",
    "    display_handle = args['display_handle']\n",
    "\n",
    "    ## environment\n",
    "    env = args['env']\n",
    "    obv = env.reset()\n",
    "    directions = ['N', 'E', 'S', 'W']\n",
    "    # if RENDER_MAZE:\n",
    "    #     env.render()\n",
    "\n",
    "    ## additional parameter initialization\n",
    "    total_steps = 0\n",
    "    cumulative_reward_array = []\n",
    "    num_steps_array = []\n",
    "\n",
    "    ## main loop\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        new_obv = env.reset()\n",
    "        # truncation is handled by the gym environment\n",
    "        for step in range(MAX_STEPS_TOTAL):\n",
    "            total_steps += 1\n",
    "            old_obv = new_obv\n",
    "            # flatten the states for the agent\n",
    "            old_obv = gym.spaces.utils.flatten(env.observation_space, old_obv)\n",
    "            action = agent.select_action(old_obv)\n",
    "            if np.any(action ==np.array([0,1,2,3])):\n",
    "                action = int(action)  # needs to be explicitly converted to int\n",
    "\n",
    "            if verbose > 1:\n",
    "                print('Selected action: ', action)\n",
    "            new_obv, reward, terminated, truncated, info = env.step(action)\n",
    "            agent.terminated = terminated\n",
    "            agent.truncated = truncated\n",
    "            agent.info = info\n",
    "            if RENDER_MAZE and episode % RENDER_EVERY == 0:\n",
    "                frame = env.render(mode=\"rgb_array\")\n",
    "                # ipython display DOES NOT support rgb_array which should literally be bitmap\n",
    "                # turns out they dont support bitmap either\n",
    "                # bgr_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                bgr_frame = frame[:, :, ::-1]\n",
    "                bgr_frame = cv2.resize(bgr_frame, (320, 320))\n",
    "                _, frame_png = cv2.imencode('.png', bgr_frame)\n",
    "                frame_bytes = frame_png.tobytes()\n",
    "                display_handle.update(display.Image(data=frame_bytes))\n",
    "            new_obv = gym.spaces.utils.flatten(env.observation_space, new_obv)\n",
    "            agent.update_step(old_obv, action, reward, new_obv)\n",
    "            if terminated or truncated:\n",
    "                print(f'Episode {episode} finished after {int(agent.num_steps)} steps with total reward {agent.cumulative_reward}')\n",
    "                cumulative_reward_array.append(agent.cumulative_reward)\n",
    "                num_steps_array.append(agent.num_steps)\n",
    "                agent.update_episode(terminated, truncated)\n",
    "                break\n",
    "            if total_steps >= MAX_STEPS_TOTAL:\n",
    "                break\n",
    "        if total_steps >= MAX_STEPS_TOTAL:\n",
    "            break\n",
    "        \n",
    "    plt.plot(cumulative_reward_array)\n",
    "    plt.title(f'Cumulative reward per episode for agent {agent.__class__.__name__}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative reward')\n",
    "    plt.show()\n",
    "\n",
    "    # plot against number of steps spent\n",
    "    num_steps_cumulative = np.cumsum(num_steps_array)\n",
    "    plt.plot(num_steps_cumulative, cumulative_reward_array)\n",
    "    plt.title(f'Cumulative reward against number of steps for agent {agent.__class__.__name__}')\n",
    "    plt.xlabel('Number of steps')\n",
    "    plt.ylabel('Cumulative reward')\n",
    "    plt.show()\n",
    "    return cumulative_reward_array, num_steps_array\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
