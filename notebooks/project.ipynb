{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "You can do docker, colab or local installation. colab may run into time limit issues. For local, you may need wsl on windows since the oscilator has some c code that dont like Windows.\n",
    "### Docker\n",
    "Install docker and run the `run_docker.sh` script. This will pull the container and run jupyter server in the container, and you shall be good to go.\n",
    "### Colab\n",
    "Use the block below. I removed the rl-zoo part comparing to the lab module. You can add it back with `%pip install --ignore-installed rl-zoo3==2.0.0` most likely (works in docker, not tested for colab). Do not forget to save a separate copy of the notebook to your drive.\n",
    "### Local\n",
    "Use the colab block, except the things you clone will be in the current folder or your specified folder instead of /content, and you want to use conda to manage environments most likely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For google colab\n",
    "Do NOT restart kernel(runtime/session) as google prompted DURING the execution of the following cell. \n",
    "\n",
    "Restart the kernel(runtime/session) AFTER the execution of the following cell is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!apt-get update \n",
    "!apt-get install -y software-properties-common\n",
    "!apt-get update && apt-get install swig cmake ffmpeg freeglut3-dev xvfb\n",
    "%pip install gymnasium\n",
    "%pip install --ignore-installed rl-zoo3==2.0.0\n",
    "%cd /content/\n",
    "!git clone https://github.com/yusenz/gym-maze.git\n",
    "%cd /content/gym-maze\n",
    "%pip install .\n",
    "%cd /content/\n",
    "%pip install opencv-python-headless\n",
    "%pip install scikit-learn==0.23.2\n",
    "%pip install tvb-library\n",
    "%pip install tvb-framework\n",
    "%pip install tensorflow[and-cuda]==2.9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import notice\n",
    "Since a lot of the imports rely on files in this repo, you may need to clone the repo, cd to the repo root and run the notebook from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/yusenz/RL_course.git\n",
    "%cd /content/RL_course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boilerplate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent framework\n",
    "This time, I will just use the custom agent framework as an example. You may want to follow some code elsewhere that works with gym environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "class BaseAgent:\n",
    "    def __init__(self, env, verbose=1, ran_seed=42):\n",
    "        self.env = env\n",
    "        # random seed is only set once when the agent is initialized\n",
    "        self.env.seed(ran_seed)\n",
    "        self.env.action_space.seed(ran_seed+1)  # why isnt this set at env.seed?\n",
    "        self.env.observation_space.seed(ran_seed+2)\n",
    "        self.random_state = np.random.RandomState(ran_seed+3)\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "        self.verbose = verbose\n",
    "        self.cumulative_reward = 0\n",
    "        self.num_steps = 0\n",
    "    def select_action(self, state):\n",
    "        raise NotImplementedError\n",
    "    def update_step(self, reward: float):\n",
    "        self.cumulative_reward += reward\n",
    "        self.num_steps += 1\n",
    "    def update_episode(self):\n",
    "        self.reset_episode()\n",
    "    def update_rollout(self):\n",
    "        if self.verbose > 0:\n",
    "            print('update_rollout in base class is called, nothing is changed')\n",
    "    def update_replay(self):\n",
    "        if self.verbose > 0:\n",
    "            print('update_replay in base class is called, nothing is changed')\n",
    "    def reset_episode(self):\n",
    "        self.cumulative_reward = 0\n",
    "        self.num_steps = 0\n",
    "\n",
    "class RandomAgent(BaseAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.cumulative_reward = 0\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def select_action(self, state):\n",
    "        action = self.action_space.sample()\n",
    "        if self.verbose > 1:\n",
    "            print('Random agent selected action: ', action)\n",
    "        return action\n",
    "    def update_step(self, old_state, action, reward, new_state):\n",
    "        super().update_step(reward)\n",
    "    def update_episode(self, terminated, truncated):\n",
    "        if self.verbose > 0:\n",
    "            if terminated:\n",
    "                print('Episode terminated')\n",
    "            if truncated:\n",
    "                print('Episode truncated')\n",
    "        super().update_episode()\n",
    "    def update_rollout(self):\n",
    "        pass\n",
    "    def update_replay(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage of rl cardiac\n",
    "See the readme under `rl_cardiac` for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from rl_cardiac.tcn_model import TCN_config\n",
    "from rl_cardiac.cardiac_model import CardiacModel_Env\n",
    "tcn_model = TCN_config(rat_type)\n",
    "env = CardiacModel_Env(tcn_model, rat_type)\n",
    "# noise level is set to 0 by default, should be changed to see if your agent can handle noise once it works well without noise\n",
    "# env = CardiacModel_Env(tcn_model, rat_type, noise_level) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "policy_kwargs = dict(net_arch=[64])\n",
    "model = PPO(\"MlpPolicy\", env, verbose = 1,  learning_rate = 0.002, n_steps=128, batch_size = 4, n_epochs=4, clip_range = 0.2, gamma = 0.95, vf_coef =1, ent_coef = 0.005, policy_kwargs = policy_kwargs )\n",
    "env.seed = 42\n",
    "env.reset()\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage of rl dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import rl_dbs.gym_oscillator\n",
    "import rl_dbs.gym_oscillator.envs\n",
    "import rl_dbs.oscillator_cpp\n",
    "env = rl_dbs.gym_oscillator.envs.oscillatorEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "policy_kwargs = dict(net_arch=[64])\n",
    "model = PPO(\"MlpPolicy\", env, verbose = 1,  learning_rate = 0.002, n_steps=128, batch_size = 4, n_epochs=4, clip_range = 0.2, gamma = 0.95, vf_coef =1, ent_coef = 0.005, policy_kwargs = policy_kwargs )\n",
    "env.seed = 42\n",
    "env.reset()\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage of TVB Epileptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from TVB.tvb_wrapper import TVBWrapper\n",
    "env = TVBWrapper(timestep=10, history_len=2000, max_len=6000, dt=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "policy_kwargs = dict(net_arch=[64])\n",
    "model = PPO(\"MlpPolicy\", env, verbose = 1,  learning_rate = 0.002, n_steps=128, batch_size = 4, n_epochs=4, clip_range = 0.2, gamma = 0.95, vf_coef =1, ent_coef = 0.005, policy_kwargs = policy_kwargs )\n",
    "env.seed = 42\n",
    "env.reset()\n",
    "model.learn(total_timesteps=10000)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
